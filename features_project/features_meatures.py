import os
import sys
import numpy as np
import pandas as pd
import seaborn as sns
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor, LGBMClassifier
from catboost import CatBoostRegressor, CatBoostClassifier
from xgboost import XGBRegressor, XGBClassifier, plot_importance
from statsmodels.stats.outliers_influence import variance_inflation_factor




class FeatureAnalysis:
    def __init__(self,df,features=None,model_cls=None,target_col="Churn"):
        self.df = df
        self.features = features
        self.model_cls = model_cls
        self.target_col = target_col

    ##åˆ†æç‰¹å¾å€¼é‡è¦æ€§
    def plot_feature_importance(self):
        if self.model_cls == XGBRegressor:
           model = XGBRegressor(n_estimators=100, random_state=42)
           model.fit(self.df[self.features], self.df[self.target_col])
           plot_importance(model, max_num_features=15)
           plt.title("Top 15 Feature Importance (XGB)")
           plt.show()

        elif self.model_cls == LGBMRegressor:
            model = LGBMRegressor(n_estimators=100, random_state=42)
            model.fit(self.df[self.features], self.df[self.target_col])
            importance = model.feature_importances_
            feat_imp = pd.Series(importance, index=self.features)
            feat_imp.nlargest(15).plot(kind='barh')
            plt.title("Top 15 Feature Importance (LGBM)")
            plt.show()

        elif self.model_cls == CatBoostRegressor:
            model = CatBoostRegressor(verbose=0, random_state=42)
            model.fit(self.df[self.features], self.df[self.target_col])
            importance = model.get_feature_importance()
            feat_imp = pd.Series(importance, index=self.features)
            feat_imp.nlargest(15).plot(kind='barh')
            plt.title("Top 15 Feature Importance (CatBoost)")
            plt.show()

        elif self.model_cls == RandomForestRegressor:
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            model.fit(self.df[self.features], self.df[self.target_col])
            importance = model.feature_importances_
            feat_imp = pd.Series(importance, index=self.features)
            feat_imp.nlargest(15).plot(kind='barh')
            plt.title("Top 15 Feature Importance (Random Forest)")
            plt.show()

        elif self.model_cls == XGBClassifier:
            model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)
            model.fit(self.df[self.features], self.df[self.target_col])
            plot_importance(model, max_num_features=15)
            plt.title("Top 15 Feature Importance (XGBClassifier)")
            plt.show()

        elif self.model_cls == LGBMClassifier:
            model = LGBMClassifier(n_estimators=100, random_state=42)
            model.fit(self.df[self.features], self.df[self.target_col])
            importance = model.feature_importances_
            feat_imp = pd.Series(importance, index=self.features)
            feat_imp.nlargest(15).plot(kind='barh')
            plt.title("Top 15 Feature Importance (LGBMClassifier)")
            plt.show()

        elif self.model_cls == CatBoostClassifier:
            model = CatBoostClassifier(verbose=0, random_state=42)
            model.fit(self.df[self.features], self.df[self.target_col])
            importance = model.get_feature_importance()
            feat_imp = pd.Series(importance, index=self.features)
            feat_imp.nlargest(15).plot(kind='barh')
            plt.title("Top 15 Feature Importance (CatBoostClassifier)")
            plt.show()

        elif self.model_cls == RandomForestClassifier:
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(self.df[self.features], self.df[self.target_col])
            importance = model.feature_importances_
            feat_imp = pd.Series(importance, index=self.features)
            feat_imp.nlargest(15).plot(kind='barh')
            plt.title("Top 15 Feature Importance (Random Forest Classifier)")
            plt.show()

        elif self.model_cls == LogisticRegression:
            model = LogisticRegression(max_iter=1000)
            model.fit(self.df[self.features], self.df[self.target_col])
            importance = model.coef_[0]
            feat_imp = pd.Series(importance, index=self.features)
            feat_imp.abs().nlargest(15).plot(kind='barh')
            plt.title("Top 15 Coefficients (Logistic Regression)")
            plt.show()

        elif self.model_cls == SVC:
            model = SVC(kernel='linear')
            model.fit(self.df[self.features], self.df[self.target_col])
            importance = model.coef_[0]
            feat_imp = pd.Series(importance, index=self.features)
            feat_imp.abs().nlargest(15).plot(kind='barh')
            plt.title("Top 15 Coefficients (SVM)")
            plt.show()

        elif self.model_cls == MLPClassifier:
            print("MLPClassifier ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§å›¾ã€‚")

    #ä¸€é”®ç”»å›¾
    def plot_feature_distribution(self):
        for col in self.features:
            plt.figure(figsize=(6, 3))
            sns.set_style("whitegrid")
            sns.histplot(self.df[col], kde=True)
            plt.title(f"{col} (skew = {self.df[col].skew():.2f})")
            plt.xlabel("Importance")
            plt.ylabel("Feature")
            plt.tight_layout()
            plt.show()
            
    #å’Œç›®æ ‡çš„ç›¸å…³æ€§
    def plot_feature_vs_target(self):
        for col in self.features:
            plt.figure(figsize=(6, 3))
            sns.set_style("whitegrid")
            sns.scatterplot(data=self.df, x=col, y=self.target_col, alpha=0.3)
            plt.xlabel("Importance")
            plt.ylabel("Feature")
            plt.title(f"{col} vs {self.target_col}")
            plt.tight_layout()
            plt.show()


    ##ç‰¹å¾å€¼ç›¸å…³æ€§åˆ†æ
    def plot_feature_correlation(self, add_features = None, threshold=0.85,verbose=True):
        """
        æ˜¾ç¤ºç‰¹å¾é—´çš„ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼Œå¹¶æ ‡è®°é«˜ç›¸å…³å¯¹ã€‚
        """
        if add_features is None:
           add_features = self.df.select_dtypes(include='number').columns
        corr_matrix = self.df[add_features].corr()
        # çƒ­åŠ›å›¾
        plt.figure(figsize=(12, 8))
        sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", square=True)
        plt.title("Feature Correlation Heatmap")
        plt.show()
        # è¾“å‡ºé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
        high_corr = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i):
                if abs(corr_matrix.iloc[i, j]) > threshold:
                    col1 = corr_matrix.columns[i]
                    col2 = corr_matrix.columns[j]
                    score = corr_matrix.iloc[i, j]
                    high_corr.append((col1, col2, round(score, 3)))
        if high_corr:
            print(" é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹ï¼ˆ|ç›¸å…³æ€§| > {})ï¼š".format(threshold))
            for col1, col2, score in high_corr:
                print(f"{col1} & {col2} â†’ ç›¸å…³ç³»æ•°: {score}")
        else:
            print("æ²¡æœ‰æ£€æµ‹åˆ°é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹")
        #è‡ªåŠ¨å¤„ç†é«˜ç›¸å…³ç‰¹å¾å€¼å¯¹
        #å–çš„æ˜¯ ç›¸å…³çŸ©é˜µçš„ä¸Šä¸‰è§’ï¼ˆå³åªçœ‹æ¯å¯¹ç»„åˆä¸€æ¬¡ï¼Œè·³è¿‡å¯¹ç§°éƒ¨åˆ†ï¼‰,é å³çš„åˆ—ä¼˜å…ˆè¢«åˆ ï¼Œé å·¦çš„åˆ—ä¼˜å…ˆä¿ç•™
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper.columns if any(upper[column] > abs(threshold))]
        if verbose:
            print(f"ğŸ” æ£€æµ‹åˆ° {len(to_drop)} ä¸ªé«˜ç›¸å…³ç‰¹å¾å°†è¢«åˆ é™¤ï¼ˆé˜ˆå€¼ï¼š{threshold}ï¼‰ï¼š")
            print(to_drop)
        advanced_feature = [col for col in self.features if col not in to_drop]
        return advanced_feature


    ### VIFæ£€éªŒ
def check_multicollinearity(df, features=None, threshold=10, verbose= True):
    missing = [feat for feat in features if feat not in df.columns]
    

    """
    æ£€æŸ¥ DataFrame ä¸­æ•°å€¼å‹å˜é‡çš„å¤šé‡å…±çº¿æ€§ï¼ˆä½¿ç”¨ VIF æŒ‡æ ‡ï¼‰

    å‚æ•°:
    df : pd.DataFrame â€”â€” è¦æ£€æŸ¥çš„ç‰¹å¾æ•°æ®
    threshold : float â€”â€” åˆ¤æ–­å…±çº¿æ€§æ˜¯å¦ä¸¥é‡çš„é˜ˆå€¼ï¼ˆé€šå¸¸è®¾ä¸º 5 æˆ– 10)

    è¿”å›:
    vif_df : pd.DataFrame â€”â€” æ¯ä¸ªå˜é‡å¯¹åº”çš„ VIF å€¼
    """
    if features:
        df = df[features]  # åªä¿ç•™æŒ‡å®šçš„åˆ—

    # åªä¿ç•™æ•°å€¼å‹å˜é‡
    print("ä¼ å…¥çš„ç‰¹å¾æ•°:", len(features) if features else "None")
    print("åŸå§‹ df.shape:", df.shape)

    numeric_df = df.select_dtypes(include=["number"]).dropna()


    print("æœ€ç»ˆè¿›å…¥VIFè®¡ç®—çš„åˆ—æ•°:", numeric_df.shape[1])
    print("è¿›å…¥VIFè®¡ç®—çš„ç‰¹å¾åˆ—:", numeric_df.columns.tolist())
    # åŠ å¸¸æ•°é¡¹ç”¨äº statsmodels
    X = sm.add_constant(numeric_df)

    vif_data = {
        "feature": X.columns,
        "VIF": [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    }

    vif_df = pd.DataFrame(vif_data)

    print("ğŸ§ª æ–¹å·®è†¨èƒ€å› å­(VIF)æ£€æµ‹ç»“æœ:")
    print(vif_df[vif_df["VIF"] > threshold])

    #åˆ é™¤ç‰¹å¾å€¼é€»è¾‘ï¼Œæ¯è½®åˆ é™¤ä¸€ä¸ªæœ€å¤§çš„VIF
    while True:
        X = sm.add_constant(numeric_df)
        vif = pd.Series(
            [variance_inflation_factor(X.values, i) for i in range(X.shape[1])],
            index=X.columns
        )
        vif = vif.drop("const")

        max_vif = vif.max()
        if max_vif > threshold:
            drop_feat = vif.idxmax()
            if verbose:
                print(f"âš ï¸  åˆ é™¤é«˜VIFç‰¹å¾: {drop_feat} (VIF={max_vif:.2f})")
            numeric_df = numeric_df.drop(columns=[drop_feat])
           
        else:
            break
    print("å»æ‰ç¼ºå¤±å€¼åçš„å½¢çŠ¶:", numeric_df.shape)
    VIF_name = vif.index.tolist()
    advanced_features_ultimate = [col for col in VIF_name ]

    return advanced_features_ultimate

    



def feature_selection_by_k(features, target_col, max_k=20, rank_features=10, model_cls=RandomForestRegressor):

    results = []

    # Step 1: ç”¨è¿‡æ»¤æ³•é€‰ top max_k ä¸ªåˆå§‹ç‰¹å¾
    filter_selector = SelectKBest(score_func=f_regression, k=max_k)
    X_filtered = filter_selector.fit_transform(features, target_col)
    filtered_cols = features.columns[filter_selector.get_support()]
    X_filtered_df = features[filtered_cols]

    # Step 2: éå† kï¼ŒåµŒå…¥æ³•è¯„ä¼°æ¯ç»„å‰ k ç‰¹å¾çš„è´¡çŒ®
    for k in range(10, max_k + 1):
        X_k = X_filtered_df.iloc[:, :k]
        model = model_cls(n_estimators=50, random_state=42)
        model.fit(X_k, target_col)

        importances = model.feature_importances_
        num_features_to_rank = min(rank_features, k)
        top_k_idx = np.argsort(importances)[::-1][:num_features_to_rank]
        top_k_features = X_k.columns[top_k_idx]
        X_top_k = X_k[top_k_features]  # optional

        results.append((k, top_k_features.tolist()))

    results_df = pd.DataFrame(results, columns=["num_features", "top_features"])
    return results_df


def select_final_top_features(features, target_col, max_k=20, top_k=15, model_cls=RandomForestRegressor):
    from sklearn.feature_selection import SelectKBest, f_regression
    import numpy as np

    # Step 1: è¿‡æ»¤æ³•åˆç­›
    filter_selector = SelectKBest(score_func=f_regression, k=max_k)
    X_filtered = filter_selector.fit_transform(features, target_col)
    filtered_cols = features.columns[filter_selector.get_support()]
    X_filtered_df = features[filtered_cols]

    # Step 2: åµŒå…¥æ³•è¯„ä¼°ç‰¹å¾é‡è¦æ€§
    model = model_cls(n_estimators=100, random_state=42)
    model.fit(X_filtered_df, target_col)

    importances = model.feature_importances_
    top_idx = np.argsort(importances)[::-1][:top_k]
    top_features = X_filtered_df.columns[top_idx]
    top_features = top_features.tolist()

    return top_features
