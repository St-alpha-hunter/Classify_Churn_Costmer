from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
#from model.train_model import train_model
from sklearn.model_selection import cross_validate, KFold
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
)
import pandas as pd
import numpy as np


class evaluate:
    def __init__(self, model,X_test, y_test,target_col = "Churn"):
        self.model = model
        self.X_test = X_test
        self.y_test = y_test
        self.target_col = target_col
    


    def evaluate_regress_model(self):
        y_pred = self.model.predict(self.X_test)
        mae = mean_absolute_error(self.y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
        r2 = r2_score(self.y_test, y_pred)
        print(f"MAE: {mae:.2f}")
        print(f"RMSE: {rmse:.2f}")
        print(f"RÂ² Score: {r2:.4f}")
        return {"MAE": mae, "RMSE": rmse, "R2": r2}
    
    def evaluate_classify_model(self, average="binary"):
        y_pred = self.model.predict(self.X_test)

        # å¦‚æœæ¨¡å‹æ”¯æŒ predict_probaï¼Œè®¡ç®— AUC
        try:
            y_proba = self.model.predict_proba(self.X_test)[:, 1]
            auc = roc_auc_score(self.y_test, y_proba)
        except:
            auc = None

        acc = accuracy_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred, average=average)
        precision = precision_score(self.y_test, y_pred, average=average)
        recall = recall_score(self.y_test, y_pred, average=average)

        print(f"âœ… Accuracy : {acc:.4f}")
        print(f"âœ… F1 Score : {f1:.4f}")
        print(f"âœ… Precision: {precision:.4f}")
        print(f"âœ… Recall   : {recall:.4f}")
        if auc is not None:
            print(f"âœ… AUC      : {auc:.4f}")
        else:
            print("âš ï¸  å½“å‰æ¨¡å‹ä¸æ”¯æŒ predict_probaï¼Œæ— æ³•è®¡ç®— AUC")

        return {
            "Accuracy": acc,
            "F1": f1,
            "Precision": precision,
            "Recall": recall,
            "AUC": auc
        }

    def enhanced_cross_validate(self, model, features=None, cv=5, verbose=True, return_df=False):
        """
        æ”¯æŒ MAEã€MSEã€RÂ² çš„äº¤å‰éªŒè¯å‡½æ•°
        """
        scoring = {
            'MAE': 'neg_mean_absolute_error',
            'MSE': 'neg_mean_squared_error',
            'R2': 'r2'
        }
        kfold = KFold(n_splits=cv, shuffle=True, random_state=42)
        scores = cross_validate(self.model, features, self.target_col, cv=kfold, scoring=scoring, return_train_score=False)
        # è½¬ä¸º DataFrame æ–¹ä¾¿åå¤„ç†
        df_scores = pd.DataFrame(scores)
        if verbose:
            print(f"ğŸ“Š {cv}-æŠ˜äº¤å‰éªŒè¯ç»“æœ:")
            for metric in ['MAE', 'MSE', 'R2']:
                test_col = f'test_{metric}'
                mean = df_scores[test_col].mean()
                std = df_scores[test_col].std()
                if metric != 'R2':  # MAE, MSE æ˜¯è´Ÿçš„
                    mean, std = -mean, std
                print(f"ğŸ”¹ {metric:<4}: {mean:.4f} Â± {std:.4f}")
        if return_df:
            return df_scores
        else:
            return {metric: df_scores[f'test_{metric}'] for metric in scoring}
    ##æŒ‡å®šè¯„ä¼°æ ‡å‡†ï¼ˆå›å½’æ¨¡å‹æœ€å¸¸è§å¦‚ MAEã€RMSEã€RÂ²ï¼‰